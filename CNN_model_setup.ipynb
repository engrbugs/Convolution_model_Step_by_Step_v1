{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36fb3c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ce4c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
    "    as illustrated in Figure 1.\n",
    "    \n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2 * pad, n_W + 2 * pad, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    #(≈ 1 line)\n",
    "    # X_pad = None\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant', constant_values=0)\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12d9c6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape =\n",
      " (4, 3, 3, 2)\n",
      "x_pad.shape =\n",
      " (4, 9, 9, 2)\n",
      "x[1,1] =\n",
      " [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_pad[1,1] =\n",
      " [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c500106c40>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_pad = zero_pad(x, 3)\n",
    "print (\"x.shape =\\n\", x.shape)\n",
    "print (\"x_pad.shape =\\n\", x_pad.shape)\n",
    "print (\"x[1,1] =\\n\", x[1, 1])\n",
    "print (\"x_pad[1,1] =\\n\", x_pad[1, 1])\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0, :, :, 0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb56cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "    of the previous layer.\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- a scalar value, the result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "\n",
    "    #(≈ 3 lines of code)\n",
    "    # Element-wise product between a_slice_prev and W. Do not add the bias yet.\n",
    "    # s = None\n",
    "    # Sum over all entries of the volume s.\n",
    "    # Z = None\n",
    "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
    "    # Z = None\n",
    "    # YOUR CODE STARTS HERE\n",
    "    s = np.multiply(a_slice_prev, W)\n",
    "    Z = np.sum(s)\n",
    "    Z = Z + float(b)\n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "762d08d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -6.999089450680221\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)\n",
    "assert (type(Z) == np.float64), \"You must cast the output to numpy float 64\"\n",
    "assert np.isclose(Z, -6.999089450680221), \"Wrong value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa453d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "\n",
    "    n_H = int((n_H_prev - f + (2 * pad)) / stride + 1)\n",
    "    n_W = int((n_W_prev - f + (2 * pad)) / stride + 1)\n",
    "\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "\n",
    "    for i in range(m):\n",
    "        a_prev_pad = A_prev[i]\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    vert_start = stride * h\n",
    "                    vert_end = stride * h + f\n",
    "                    horiz_start = stride * w\n",
    "                    horiz_end = stride * w + f\n",
    "                    a_slice_prev = A_prev_pad[i, vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:, :, :, c], b[:, :, :, c])\n",
    "     \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1f04a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean =\n",
      " 0.5511276474566768\n",
      "Z[0,2,1] =\n",
      " [-2.17796037  8.07171329 -0.5772704   3.36286738  4.48113645 -2.89198428\n",
      " 10.99288867  3.03171932]\n",
      "cache_conv[0][1][2][3] =\n",
      " [-1.1191154   1.9560789  -0.3264995  -1.34267579]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 7, 4)\n",
    "W = np.random.randn(3, 3, 4, 8)\n",
    "b = np.random.randn(1, 1, 1, 8)\n",
    "hparameters = {\"pad\" : 1,\n",
    "               \"stride\": 2}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "z_mean = np.mean(Z)\n",
    "z_0_2_1 = Z[0, 2, 1]\n",
    "cache_0_1_2_3 = cache_conv[0][1][2][3]\n",
    "print(\"Z's mean =\\n\", z_mean)\n",
    "print(\"Z[0,2,1] =\\n\", z_0_2_1)\n",
    "print(\"cache_conv[0][1][2][3] =\\n\", cache_0_1_2_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4fb3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: pool_forward\n",
    "\n",
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
    "    \"\"\"\n",
    "    # Retrieve dimensions from the input shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    for i in range(m):                         \n",
    "        for h in range(n_H):            \n",
    "            for w in range(n_W):          \n",
    "                for c in range (n_C):\n",
    "                    \n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "04aafb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A.shape = (2, 3, 3, 3)\n",
      "A[1, 1] =\n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "mode = average\n",
      "A.shape = (2, 3, 3, 3)\n",
      "A[1, 1] =\n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Case 1: stride of 1\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 5, 3)\n",
    "hparameters = {\"stride\" : 1, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[1, 1] =\\n\", A[1, 1])\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[1, 1] =\\n\", A[1, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "74d7c0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A.shape = (2, 2, 2, 3)\n",
      "A[0] =\n",
      " [[[0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]]]\n",
      "\n",
      "mode = average\n",
      "A.shape = (2, 2, 2, 3)\n",
      "A[1] =\n",
      " [[[0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Case 2: stride of 2\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 5, 3)\n",
    "hparameters = {\"stride\" : 2, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[0] =\\n\", A[0])\n",
    "print()\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[1] =\\n\", A[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f45aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):     \n",
    "        \n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):              \n",
    "            for w in range(n_W):            \n",
    "                for c in range(n_C):     \n",
    "                    \n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5731b726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 1.4524377775388075\n",
      "dW_mean = 1.7269914583139097\n",
      "db_mean = 7.839232564616838\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "# We'll run conv_forward to initialize the 'Z' and 'cache_conv\",\n",
    "# which we'll use to test the conv_backward function\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10, 4, 4, 3)\n",
    "W = np.random.randn(2, 2, 3, 8)\n",
    "b = np.random.randn(1, 1, 1, 8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 2}\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "\n",
    "# Test conv_backward\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))\n",
    "\n",
    "assert type(dA) == np.ndarray, \"Output must be a np.ndarray\"\n",
    "assert type(dW) == np.ndarray, \"Output must be a np.ndarray\"\n",
    "assert type(db) == np.ndarray, \"Output must be a np.ndarray\"\n",
    "assert dA.shape == (10, 4, 4, 3), f\"Wrong shape for dA  {dA.shape} != (10, 4, 4, 3)\"\n",
    "assert dW.shape == (2, 2, 3, 8), f\"Wrong shape for dW {dW.shape} != (2, 2, 3, 8)\"\n",
    "assert db.shape == (1, 1, 1, 8), f\"Wrong shape for db {db.shape} != (1, 1, 1, 8)\"\n",
    "assert np.isclose(np.mean(dA), 1.4524377), \"Wrong values for dA\"\n",
    "assert np.isclose(np.mean(dW), 1.7269914), \"Wrong values for dW\"\n",
    "assert np.isclose(np.mean(db), 7.8392325), \"Wrong values for db\"\n",
    "\n",
    "print(\"\\033[92m All tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09e697a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    \"\"\"    \n",
    "    # (≈1 line)\n",
    "    # mask = None\n",
    "    # YOUR CODE STARTS HERE\n",
    "    mask = (x == np.max(x))\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a2867dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "mask =  [[ True False False]\n",
      " [False False False]]\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(2, 3)\n",
    "mask = create_mask_from_window(x)\n",
    "print('x = ', x)\n",
    "print(\"mask = \", mask)\n",
    "\n",
    "x = np.array([[-1, 2, 3],\n",
    "              [2, -3, 2],\n",
    "              [1, 5, -2]])\n",
    "\n",
    "y = np.array([[False, False, False],\n",
    "     [False, False, False],\n",
    "     [False, True, False]])\n",
    "mask = create_mask_from_window(x)\n",
    "\n",
    "assert type(mask) == np.ndarray, \"Output must be a np.ndarray\"\n",
    "assert mask.shape == x.shape, \"Input and output shapes must match\"\n",
    "assert np.allclose(mask, y), \"Wrong output. The True value must be at position (2, 1)\"\n",
    "\n",
    "print(\"\\033[92m All tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a013112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar\n",
    "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"    \n",
    "    # Retrieve dimensions from shape (≈1 line)\n",
    "    # (n_H, n_W) = None\n",
    "    \n",
    "    # Compute the value to distribute on the matrix (≈1 line)\n",
    "    # average = None\n",
    "    \n",
    "    # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
    "    # a = None\n",
    "    # YOUR CODE STARTS HERE\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    average = dz / (n_H * n_W)\n",
    "    \n",
    "    a = np.ones(shape) * average\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7bbb6d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributed value = [[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "a = distribute_value(2, (2, 2))\n",
    "print('distributed value =', a)\n",
    "\n",
    "\n",
    "assert type(a) == np.ndarray, \"Output must be a np.ndarray\"\n",
    "assert a.shape == (2, 2), f\"Wrong shape {a.shape} != (2, 2)\"\n",
    "assert np.sum(a) == 2, \"Values must sum to 2\"\n",
    "\n",
    "a = distribute_value(100, (10, 10))\n",
    "assert type(a) == np.ndarray, \"Output must be a np.ndarray\"\n",
    "assert a.shape == (10, 10), f\"Wrong shape {a.shape} != (10, 10)\"\n",
    "assert np.sum(a) == 100, \"Values must sum to 100\"\n",
    "\n",
    "print(\"\\033[92m All tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b2c8647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    # Retrieve information from cache (≈1 line)\n",
    "    # (A_prev, hparameters) = None\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\" (≈2 lines)\n",
    "    # stride = None\n",
    "    # f = None\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
    "    # m, n_H_prev, n_W_prev, n_C_prev = None\n",
    "    # m, n_H, n_W, n_C = None\n",
    "    \n",
    "    # Initialize dA_prev with zeros (≈1 line)\n",
    "    # dA_prev = None\n",
    "    \n",
    "    # for i in range(None): # loop over the training examples\n",
    "        \n",
    "        # select training example from A_prev (≈1 line)\n",
    "        # a_prev = None\n",
    "        \n",
    "        # for h in range(n_H):                   # loop on the vertical axis\n",
    "            # for w in range(n_W):               # loop on the horizontal axis\n",
    "                # for c in range(n_C):           # loop over the channels (depth)\n",
    "        \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    # vert_start = None\n",
    "                    # vert_end = None\n",
    "                    # horiz_start = None\n",
    "                    # horiz_end = None\n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    # if mode == \"max\":\n",
    "                        \n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
    "                        # a_prev_slice = None\n",
    "                        \n",
    "                        # Create the mask from a_prev_slice (≈1 line)\n",
    "                        # mask = None\n",
    "\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
    "                        # dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += None\n",
    "                        \n",
    "                    # elif mode == \"average\":\n",
    "                        \n",
    "                        # Get the value da from dA (≈1 line)\n",
    "                        # da = None\n",
    "                        \n",
    "                        # Define the shape of the filter as fxf (≈1 line)\n",
    "                        # shape = None\n",
    "\n",
    "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)\n",
    "                        # dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += None\n",
    "    # YOUR CODE STARTS HERE\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    dA_prev = np.zeros(A_prev.shape)\n",
    "    \n",
    "    for i in range(m):                    \n",
    "        \n",
    "        a_prev = A_prev[i,:,:,:]\n",
    "        \n",
    "        for h in range(n_H):          \n",
    "            for w in range(n_W):        \n",
    "                for c in range(n_C):       \n",
    "                    \n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    if mode == \"max\":\n",
    "                        \n",
    "                        a_prev_slice = a_prev[vert_start: vert_end, horiz_start: horiz_end, c]\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += np.multiply(mask, dA[i, h, w, c])\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        \n",
    "                        da = dA[i, h, w, c]\n",
    "                        shape = (f, f)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec0953be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4, 2, 2)\n",
      "(5, 5, 3, 2)\n",
      "mode = max\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev1[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev2[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3caae663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4, 2, 2)\n",
      "(5, 5, 3, 2)\n",
      "mode = max\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev1[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev2[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(A.shape)\n",
    "print(cache[0].shape)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev1 = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev1[1,1] = ', dA_prev1[1, 1])  \n",
    "print()\n",
    "dA_prev2 = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev2[1,1] = ', dA_prev2[1, 1]) \n",
    "\n",
    "assert type(dA_prev1) == np.ndarray, \"Wrong type\"\n",
    "assert dA_prev1.shape == (5, 5, 3, 2), f\"Wrong shape {dA_prev1.shape} != (5, 5, 3, 2)\"\n",
    "assert np.allclose(dA_prev1[1, 1], [[0, 0], \n",
    "                                    [ 5.05844394, -1.68282702],\n",
    "                                    [ 0, 0]]), \"Wrong values for mode max\"\n",
    "assert np.allclose(dA_prev2[1, 1], [[0.08485462,  0.2787552], \n",
    "                                    [1.26461098, -0.25749373], \n",
    "                                    [1.17975636, -0.53624893]]), \"Wrong values for mode average\"\n",
    "print(\"\\033[92m All tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db269e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc8a08bc26480cfc6a8709afa5bb2481130464758e203bd365a9579451215a1d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
